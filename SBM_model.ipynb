{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "train_data=[[0.831, 0.530, 0.703],[0.530, 0.703,0.578],[0.831, 0.531, 0.703]]\n",
        "label_data=[[0.578],[0.561],[0.593]]\n",
        "label_data_temp=[[0.578, 0.561]]"
      ],
      "metadata": {
        "id": "NOWTYft1Zioj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X):\n",
        "  train_data = []\n",
        "  label_data = []\n",
        "  label_data_temp = []\n",
        "  for i in range(len(X)):\n",
        "    #train_data\n",
        "    train_data.append(X[i][:3])\n",
        "    train_data.append(X[i][1:4])\n",
        "\n",
        "    #label_data\n",
        "    label_data.append(X[i][3:4])\n",
        "    label_data.append(X[i][4:])\n",
        "\n",
        "    #label_data_temp\n",
        "    label_data_temp.append(X[i][3:])\n",
        "\n",
        "  print(train_data)\n",
        "  print(label_data)\n",
        "  print(label_data_temp)\n",
        "\n",
        "preprocess(x_4_NA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_-dES2KbAzZ",
        "outputId": "0f0dfcbf-9157-4351-b7cd-6e8e1765fc95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.781, 0.811, 0.853], [0.811, 0.853, 0.878], [0.516, 0.53, 0.531], [0.53, 0.531, 0.555], [0.636, 0.649, 0.717], [0.649, 0.717, 0.636], [0.578, 0.578, 0.593], [0.578, 0.593, 0.656], [0.552, 0.561, 0.603], [0.561, 0.603, 0.609]]\n",
            "[[0.878], [0.781, 0.811, 0.853, 0.878], [0.555], [0.613, 0.531, 0.555, 0.613], [0.636], [0.649, 0.717, 0.75, 0.765], [0.656], [0.686, 0.695, 0.721, 0.721], [0.609], [0.63, 0.656, 0.665, 0.665]]\n",
            "[[0.878, 0.781, 0.811, 0.853, 0.878], [0.555, 0.613, 0.531, 0.555, 0.613], [0.636, 0.649, 0.717, 0.75, 0.765], [0.656, 0.686, 0.695, 0.721, 0.721], [0.609, 0.63, 0.656, 0.665, 0.665]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_preprocess(X):\n",
        "  #train_data\n",
        "  for i in range(len(X)):\n",
        "    train_data.append(X[i])\n",
        "\n",
        "  #train_label_data\n",
        "  temp = [[0], [0.001], [0.01], [0.1], [1]]\n",
        "  for i in range(len(temp)):\n",
        "    train_label_data.append(temp[i])\n",
        "\n",
        "train_data = []\n",
        "train_label_data = []\n",
        "\n",
        "#4\n",
        "train_preprocess(jeju1_4_NA)\n",
        "\n",
        "print(train_data)\n",
        "print(train_label_data)\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(train_label_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4BHDlq840sU",
        "outputId": "9bf3706e-82e4-465b-be7b-a88d05312e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.781, 0.811, 0.853, 0.878, 0.781, 0.811, 0.853, 0.878], [0.516, 0.53, 0.531, 0.555, 0.613, 0.531, 0.555, 0.613], [0.636, 0.649, 0.717, 0.636, 0.649, 0.717, 0.75, 0.765], [0.578, 0.578, 0.593, 0.656, 0.686, 0.695, 0.721, 0.721], [0.552, 0.561, 0.603, 0.609, 0.63, 0.656, 0.665, 0.665]]\n",
            "[[0], [0.001], [0.01], [0.1], [1]]\n",
            "5\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_preprocess(X):\n",
        "  #test_data\n",
        "  for i in range(len(X)):\n",
        "    test_data.append(X[i])\n",
        "\n",
        "  #test_label_data\n",
        "  temp = [[0], [0.001], [0.01], [0.1], [1]]\n",
        "  for i in range(len(temp)):\n",
        "    test_label_data.append(temp[i])\n",
        "\n",
        "test_data = []\n",
        "test_label_data = []\n",
        "\n",
        "\n",
        "#4\n",
        "test_preprocess(jeju1_4_A)\n",
        "test_preprocess(jeju2_4_NA)\n",
        "test_preprocess(normal_4_NA)\n",
        "\n",
        "print(test_data)\n",
        "print(test_label_data)\n",
        "\n",
        "print(len(test_data))\n",
        "print(len(test_label_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1259VsXA8V2n",
        "outputId": "39fc50e2-bccc-4f3a-809d-34ecbe469995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.547, 0.549, 0.553, 0.547, 0.549, 0.553, 0.651, 0.651], [0.52, 0.536, 0.52, 0.536, 0.554, 0.564, 0.59, 0.59], [0.551, 0.571, 0.551, 0.571, 0.604, 0.649, 0.604, 0.649], [0.55, 0.55, 0.553, 0.559, 0.579, 0.632, 0.634, 0.636], [0.555, 0.562, 0.555, 0.562, 0.604, 0.625, 0.604, 0.625], [0.605, 0.758, 0.817, 0.83, 0.845, 0.715, 0.654, 0.69], [0.685, 0.664, 0.606, 0.543, 0.572, 0.565, 0.524, 0.531], [0.701, 0.613, 0.67, 0.633, 0.616, 0.683, 0.649, 0.653], [0.673, 0.629, 0.704, 0.689, 0.617, 0.586, 0.644, 0.695], [0.661, 0.63, 0.603, 0.561, 0.596, 0.62, 0.581, 0.552], [0.781, 0.781, 0.724, 0.726, 0.683, 0.811, 0.682, 0.773], [0.593, 0.678, 0.679, 0.695, 0.718, 0.697, 0.726, 0.722], [0.693, 0.696, 0.664, 0.682, 0.676, 0.677, 0.663, 0.744], [0.681, 0.681, 0.754, 0.742, 0.712, 0.67, 0.742, 0.713], [0.686, 0.656, 0.656, 0.667, 0.649, 0.661, 0.686, 0.741]]\n",
            "[[0], [0.001], [0.01], [0.1], [1], [0], [0.001], [0.01], [0.1], [1], [0], [0.001], [0.01], [0.1], [1]]\n",
            "15\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "cwL4fWG-06i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = layers.Input((8,))\n",
        "\n",
        "h = layers.Dense(128)(x)\n",
        "h = layers.Dense(128)(h)\n",
        "h = layers.Dense(128)(h)\n",
        "h = layers.Dense(128)(h)\n",
        "h = layers.Dense(128)(h)\n",
        "\n",
        "y = layers.Dense(1, activation='sigmoid')(h) #sigmoid가 output layer에서 사용됐을 경우 2가지 카테고리로 분류 -> output 1개 필요"
      ],
      "metadata": {
        "id": "K-Ou55UBO6Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Model(x, y)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "batch_size=2 #batch: weight값이 한 번 업데이트될 때 사용하는 sample size\n",
        "epochs=300 #epochs:학습의 횟수\n",
        "model.fit(train_data, train_label_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(test_data, test_label_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cN38sCtPdxH",
        "outputId": "2a4cdbe4-ce32-4a69-efbf-8c77810abc2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "3/3 [==============================] - 1s 134ms/step - loss: 0.1973 - val_loss: 0.1548\n",
            "Epoch 2/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1590 - val_loss: 0.1529\n",
            "Epoch 3/300\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.1584 - val_loss: 0.1566\n",
            "Epoch 4/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1621 - val_loss: 0.1553\n",
            "Epoch 5/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1502 - val_loss: 0.1513\n",
            "Epoch 6/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1489 - val_loss: 0.1544\n",
            "Epoch 7/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1685 - val_loss: 0.1660\n",
            "Epoch 8/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1630 - val_loss: 0.1602\n",
            "Epoch 9/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1493 - val_loss: 0.1513\n",
            "Epoch 10/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1541 - val_loss: 0.1538\n",
            "Epoch 11/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1566 - val_loss: 0.1595\n",
            "Epoch 12/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1577 - val_loss: 0.1581\n",
            "Epoch 13/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1590 - val_loss: 0.1562\n",
            "Epoch 14/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1603 - val_loss: 0.1504\n",
            "Epoch 15/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1468 - val_loss: 0.1509\n",
            "Epoch 16/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1511 - val_loss: 0.1511\n",
            "Epoch 17/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1461 - val_loss: 0.1540\n",
            "Epoch 18/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1611 - val_loss: 0.1628\n",
            "Epoch 19/300\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.1611 - val_loss: 0.1614\n",
            "Epoch 20/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1534 - val_loss: 0.1545\n",
            "Epoch 21/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1502 - val_loss: 0.1502\n",
            "Epoch 22/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1432 - val_loss: 0.1521\n",
            "Epoch 23/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1525 - val_loss: 0.1569\n",
            "Epoch 24/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1632 - val_loss: 0.1588\n",
            "Epoch 25/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1495 - val_loss: 0.1516\n",
            "Epoch 26/300\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1592 - val_loss: 0.1542\n",
            "Epoch 27/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1499 - val_loss: 0.1541\n",
            "Epoch 28/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1598 - val_loss: 0.1508\n",
            "Epoch 29/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1489 - val_loss: 0.1515\n",
            "Epoch 30/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1468 - val_loss: 0.1556\n",
            "Epoch 31/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1552 - val_loss: 0.1573\n",
            "Epoch 32/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1517 - val_loss: 0.1541\n",
            "Epoch 33/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1452 - val_loss: 0.1507\n",
            "Epoch 34/300\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.1420 - val_loss: 0.1513\n",
            "Epoch 35/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1447 - val_loss: 0.1542\n",
            "Epoch 36/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1495 - val_loss: 0.1554\n",
            "Epoch 37/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1533 - val_loss: 0.1561\n",
            "Epoch 38/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1527 - val_loss: 0.1543\n",
            "Epoch 39/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1509 - val_loss: 0.1522\n",
            "Epoch 40/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1446 - val_loss: 0.1506\n",
            "Epoch 41/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1437 - val_loss: 0.1530\n",
            "Epoch 42/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1451 - val_loss: 0.1585\n",
            "Epoch 43/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1523 - val_loss: 0.1658\n",
            "Epoch 44/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1613 - val_loss: 0.1714\n",
            "Epoch 45/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1676 - val_loss: 0.1731\n",
            "Epoch 46/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1664 - val_loss: 0.1647\n",
            "Epoch 47/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1527 - val_loss: 0.1545\n",
            "Epoch 48/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1617 - val_loss: 0.1500\n",
            "Epoch 49/300\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1464 - val_loss: 0.1504\n",
            "Epoch 50/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1466 - val_loss: 0.1509\n",
            "Epoch 51/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1476 - val_loss: 0.1516\n",
            "Epoch 52/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1505 - val_loss: 0.1514\n",
            "Epoch 53/300\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.1520 - val_loss: 0.1504\n",
            "Epoch 54/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1454 - val_loss: 0.1509\n",
            "Epoch 55/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1469 - val_loss: 0.1508\n",
            "Epoch 56/300\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.1469 - val_loss: 0.1501\n",
            "Epoch 57/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1523 - val_loss: 0.1509\n",
            "Epoch 58/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1479 - val_loss: 0.1500\n",
            "Epoch 59/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1441 - val_loss: 0.1501\n",
            "Epoch 60/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1439 - val_loss: 0.1500\n",
            "Epoch 61/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1475 - val_loss: 0.1500\n",
            "Epoch 62/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1481 - val_loss: 0.1512\n",
            "Epoch 63/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1478 - val_loss: 0.1521\n",
            "Epoch 64/300\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.1446 - val_loss: 0.1508\n",
            "Epoch 65/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1447 - val_loss: 0.1501\n",
            "Epoch 66/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1448 - val_loss: 0.1519\n",
            "Epoch 67/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1442 - val_loss: 0.1525\n",
            "Epoch 68/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1454 - val_loss: 0.1523\n",
            "Epoch 69/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1466 - val_loss: 0.1527\n",
            "Epoch 70/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1473 - val_loss: 0.1510\n",
            "Epoch 71/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1464 - val_loss: 0.1507\n",
            "Epoch 72/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1407 - val_loss: 0.1502\n",
            "Epoch 73/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1477 - val_loss: 0.1527\n",
            "Epoch 74/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1448 - val_loss: 0.1511\n",
            "Epoch 75/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1421 - val_loss: 0.1501\n",
            "Epoch 76/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1420 - val_loss: 0.1506\n",
            "Epoch 77/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1402 - val_loss: 0.1522\n",
            "Epoch 78/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1455 - val_loss: 0.1546\n",
            "Epoch 79/300\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.1466 - val_loss: 0.1532\n",
            "Epoch 80/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1492 - val_loss: 0.1530\n",
            "Epoch 81/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1404 - val_loss: 0.1498\n",
            "Epoch 82/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1481 - val_loss: 0.1528\n",
            "Epoch 83/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1453 - val_loss: 0.1522\n",
            "Epoch 84/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1535 - val_loss: 0.1497\n",
            "Epoch 85/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1436 - val_loss: 0.1501\n",
            "Epoch 86/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1409 - val_loss: 0.1497\n",
            "Epoch 87/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1391 - val_loss: 0.1499\n",
            "Epoch 88/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1408 - val_loss: 0.1519\n",
            "Epoch 89/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1407 - val_loss: 0.1528\n",
            "Epoch 90/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1446 - val_loss: 0.1548\n",
            "Epoch 91/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1443 - val_loss: 0.1537\n",
            "Epoch 92/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1449 - val_loss: 0.1519\n",
            "Epoch 93/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1350 - val_loss: 0.1513\n",
            "Epoch 94/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1388 - val_loss: 0.1558\n",
            "Epoch 95/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1446 - val_loss: 0.1645\n",
            "Epoch 96/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1544 - val_loss: 0.1544\n",
            "Epoch 97/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1306 - val_loss: 0.1506\n",
            "Epoch 98/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1314 - val_loss: 0.1572\n",
            "Epoch 99/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1488 - val_loss: 0.1629\n",
            "Epoch 100/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1554 - val_loss: 0.1611\n",
            "Epoch 101/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1436 - val_loss: 0.1505\n",
            "Epoch 102/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1365 - val_loss: 0.1508\n",
            "Epoch 103/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1390 - val_loss: 0.1515\n",
            "Epoch 104/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1391 - val_loss: 0.1498\n",
            "Epoch 105/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1467 - val_loss: 0.1502\n",
            "Epoch 106/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1402 - val_loss: 0.1499\n",
            "Epoch 107/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1409 - val_loss: 0.1505\n",
            "Epoch 108/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1343 - val_loss: 0.1498\n",
            "Epoch 109/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1308 - val_loss: 0.1510\n",
            "Epoch 110/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1394 - val_loss: 0.1563\n",
            "Epoch 111/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1480 - val_loss: 0.1572\n",
            "Epoch 112/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1456 - val_loss: 0.1503\n",
            "Epoch 113/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1322 - val_loss: 0.1532\n",
            "Epoch 114/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1329 - val_loss: 0.1656\n",
            "Epoch 115/300\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.1801 - val_loss: 0.1773\n",
            "Epoch 116/300\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.1592 - val_loss: 0.1501\n",
            "Epoch 117/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1348 - val_loss: 0.1565\n",
            "Epoch 118/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1445 - val_loss: 0.1627\n",
            "Epoch 119/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1476 - val_loss: 0.1596\n",
            "Epoch 120/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1442 - val_loss: 0.1565\n",
            "Epoch 121/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1396 - val_loss: 0.1519\n",
            "Epoch 122/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1487 - val_loss: 0.1539\n",
            "Epoch 123/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1514 - val_loss: 0.1567\n",
            "Epoch 124/300\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.1379 - val_loss: 0.1503\n",
            "Epoch 125/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1368 - val_loss: 0.1512\n",
            "Epoch 126/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1320 - val_loss: 0.1540\n",
            "Epoch 127/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1356 - val_loss: 0.1551\n",
            "Epoch 128/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1385 - val_loss: 0.1550\n",
            "Epoch 129/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1388 - val_loss: 0.1517\n",
            "Epoch 130/300\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.1367 - val_loss: 0.1510\n",
            "Epoch 131/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1507 - val_loss: 0.1552\n",
            "Epoch 132/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1350 - val_loss: 0.1511\n",
            "Epoch 133/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1301 - val_loss: 0.1519\n",
            "Epoch 134/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1262 - val_loss: 0.1547\n",
            "Epoch 135/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1456 - val_loss: 0.1585\n",
            "Epoch 136/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1252 - val_loss: 0.1510\n",
            "Epoch 137/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1115 - val_loss: 0.1719\n",
            "Epoch 138/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1710 - val_loss: 0.1927\n",
            "Epoch 139/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1903 - val_loss: 0.1518\n",
            "Epoch 140/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1250 - val_loss: 0.1503\n",
            "Epoch 141/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1218 - val_loss: 0.1526\n",
            "Epoch 142/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1262 - val_loss: 0.1563\n",
            "Epoch 143/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1331 - val_loss: 0.1574\n",
            "Epoch 144/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1428 - val_loss: 0.1578\n",
            "Epoch 145/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1405 - val_loss: 0.1502\n",
            "Epoch 146/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1246 - val_loss: 0.1528\n",
            "Epoch 147/300\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1497 - val_loss: 0.1667\n",
            "Epoch 148/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1479 - val_loss: 0.1546\n",
            "Epoch 149/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1354 - val_loss: 0.1532\n",
            "Epoch 150/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1241 - val_loss: 0.1576\n",
            "Epoch 151/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1321 - val_loss: 0.1587\n",
            "Epoch 152/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1365 - val_loss: 0.1584\n",
            "Epoch 153/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1365 - val_loss: 0.1524\n",
            "Epoch 154/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1254 - val_loss: 0.1516\n",
            "Epoch 155/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1263 - val_loss: 0.1523\n",
            "Epoch 156/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1264 - val_loss: 0.1524\n",
            "Epoch 157/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1285 - val_loss: 0.1566\n",
            "Epoch 158/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1264 - val_loss: 0.1545\n",
            "Epoch 159/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1318 - val_loss: 0.1546\n",
            "Epoch 160/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1397 - val_loss: 0.1621\n",
            "Epoch 161/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1425 - val_loss: 0.1544\n",
            "Epoch 162/300\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.1263 - val_loss: 0.1632\n",
            "Epoch 163/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1280 - val_loss: 0.1668\n",
            "Epoch 164/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1458 - val_loss: 0.1656\n",
            "Epoch 165/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1258 - val_loss: 0.1535\n",
            "Epoch 166/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1096 - val_loss: 0.1604\n",
            "Epoch 167/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1709 - val_loss: 0.1813\n",
            "Epoch 168/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1399 - val_loss: 0.1531\n",
            "Epoch 169/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1039 - val_loss: 0.1621\n",
            "Epoch 170/300\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.1273 - val_loss: 0.1681\n",
            "Epoch 171/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1402 - val_loss: 0.1658\n",
            "Epoch 172/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1401 - val_loss: 0.1579\n",
            "Epoch 173/300\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1251 - val_loss: 0.1539\n",
            "Epoch 174/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1252 - val_loss: 0.1528\n",
            "Epoch 175/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1193 - val_loss: 0.1532\n",
            "Epoch 176/300\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.1137 - val_loss: 0.1543\n",
            "Epoch 177/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1231 - val_loss: 0.1623\n",
            "Epoch 178/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1321 - val_loss: 0.1641\n",
            "Epoch 179/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1293 - val_loss: 0.1561\n",
            "Epoch 180/300\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.1151 - val_loss: 0.1553\n",
            "Epoch 181/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1116 - val_loss: 0.1554\n",
            "Epoch 182/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1099 - val_loss: 0.1569\n",
            "Epoch 183/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1156 - val_loss: 0.1615\n",
            "Epoch 184/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1222 - val_loss: 0.1584\n",
            "Epoch 185/300\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.1098 - val_loss: 0.1579\n",
            "Epoch 186/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1041 - val_loss: 0.1588\n",
            "Epoch 187/300\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1196 - val_loss: 0.1604\n",
            "Epoch 188/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1264 - val_loss: 0.1762\n",
            "Epoch 189/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0895 - val_loss: 0.1641\n",
            "Epoch 190/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1036 - val_loss: 0.1794\n",
            "Epoch 191/300\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.1604 - val_loss: 0.1834\n",
            "Epoch 192/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1616 - val_loss: 0.1761\n",
            "Epoch 193/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1345 - val_loss: 0.1552\n",
            "Epoch 194/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0928 - val_loss: 0.1643\n",
            "Epoch 195/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1321 - val_loss: 0.1963\n",
            "Epoch 196/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1977 - val_loss: 0.1842\n",
            "Epoch 197/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1358 - val_loss: 0.1534\n",
            "Epoch 198/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1065 - val_loss: 0.1637\n",
            "Epoch 199/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1321 - val_loss: 0.1673\n",
            "Epoch 200/300\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.1482 - val_loss: 0.1666\n",
            "Epoch 201/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1443 - val_loss: 0.1531\n",
            "Epoch 202/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1157 - val_loss: 0.1507\n",
            "Epoch 203/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1120 - val_loss: 0.1643\n",
            "Epoch 204/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1415 - val_loss: 0.1805\n",
            "Epoch 205/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1482 - val_loss: 0.1572\n",
            "Epoch 206/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1220 - val_loss: 0.1511\n",
            "Epoch 207/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1217 - val_loss: 0.1572\n",
            "Epoch 208/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1251 - val_loss: 0.1588\n",
            "Epoch 209/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1291 - val_loss: 0.1566\n",
            "Epoch 210/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1246 - val_loss: 0.1535\n",
            "Epoch 211/300\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.1185 - val_loss: 0.1520\n",
            "Epoch 212/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1113 - val_loss: 0.1574\n",
            "Epoch 213/300\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.1310 - val_loss: 0.1605\n",
            "Epoch 214/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1213 - val_loss: 0.1539\n",
            "Epoch 215/300\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.1044 - val_loss: 0.1572\n",
            "Epoch 216/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1112 - val_loss: 0.1588\n",
            "Epoch 217/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1124 - val_loss: 0.1586\n",
            "Epoch 218/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1114 - val_loss: 0.1591\n",
            "Epoch 219/300\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.1053 - val_loss: 0.1614\n",
            "Epoch 220/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1166 - val_loss: 0.1646\n",
            "Epoch 221/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0884 - val_loss: 0.1636\n",
            "Epoch 222/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1020 - val_loss: 0.1652\n",
            "Epoch 223/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1244 - val_loss: 0.1633\n",
            "Epoch 224/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1801 - val_loss: 0.2209\n",
            "Epoch 225/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1463 - val_loss: 0.1593\n",
            "Epoch 226/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1229 - val_loss: 0.1685\n",
            "Epoch 227/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1287 - val_loss: 0.1681\n",
            "Epoch 228/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1335 - val_loss: 0.1601\n",
            "Epoch 229/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1171 - val_loss: 0.1544\n",
            "Epoch 230/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1084 - val_loss: 0.1534\n",
            "Epoch 231/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1144 - val_loss: 0.1575\n",
            "Epoch 232/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1099 - val_loss: 0.1554\n",
            "Epoch 233/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1057 - val_loss: 0.1545\n",
            "Epoch 234/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1140 - val_loss: 0.1602\n",
            "Epoch 235/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1137 - val_loss: 0.1601\n",
            "Epoch 236/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1129 - val_loss: 0.1578\n",
            "Epoch 237/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1061 - val_loss: 0.1576\n",
            "Epoch 238/300\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.0985 - val_loss: 0.1605\n",
            "Epoch 239/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0931 - val_loss: 0.1590\n",
            "Epoch 240/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0894 - val_loss: 0.1615\n",
            "Epoch 241/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1100 - val_loss: 0.1660\n",
            "Epoch 242/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1142 - val_loss: 0.1618\n",
            "Epoch 243/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0988 - val_loss: 0.1664\n",
            "Epoch 244/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1079 - val_loss: 0.1627\n",
            "Epoch 245/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0708 - val_loss: 0.1941\n",
            "Epoch 246/300\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.1290 - val_loss: 0.1687\n",
            "Epoch 247/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0550 - val_loss: 0.1777\n",
            "Epoch 248/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1406 - val_loss: 0.1842\n",
            "Epoch 249/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.1636 - val_loss: 0.1803\n",
            "Epoch 250/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.1458 - val_loss: 0.1636\n",
            "Epoch 251/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0836 - val_loss: 0.1671\n",
            "Epoch 252/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1369 - val_loss: 0.2265\n",
            "Epoch 253/300\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.1755 - val_loss: 0.1572\n",
            "Epoch 254/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0867 - val_loss: 0.1551\n",
            "Epoch 255/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1000 - val_loss: 0.1561\n",
            "Epoch 256/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1003 - val_loss: 0.1553\n",
            "Epoch 257/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0919 - val_loss: 0.1570\n",
            "Epoch 258/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1008 - val_loss: 0.1596\n",
            "Epoch 259/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0871 - val_loss: 0.1573\n",
            "Epoch 260/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0909 - val_loss: 0.1583\n",
            "Epoch 261/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.0919 - val_loss: 0.1595\n",
            "Epoch 262/300\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.0894 - val_loss: 0.1605\n",
            "Epoch 263/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0847 - val_loss: 0.1646\n",
            "Epoch 264/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0898 - val_loss: 0.1612\n",
            "Epoch 265/300\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0763 - val_loss: 0.1685\n",
            "Epoch 266/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1057 - val_loss: 0.1717\n",
            "Epoch 267/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0602 - val_loss: 0.1633\n",
            "Epoch 268/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0824 - val_loss: 0.1627\n",
            "Epoch 269/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0634 - val_loss: 0.1869\n",
            "Epoch 270/300\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.0867 - val_loss: 0.1633\n",
            "Epoch 271/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.0511 - val_loss: 0.1801\n",
            "Epoch 272/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1613 - val_loss: 0.1802\n",
            "Epoch 273/300\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.1452 - val_loss: 0.1700\n",
            "Epoch 274/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0464 - val_loss: 0.2173\n",
            "Epoch 275/300\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.1285 - val_loss: 0.1622\n",
            "Epoch 276/300\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.1803 - val_loss: 0.1924\n",
            "Epoch 277/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1869 - val_loss: 0.1958\n",
            "Epoch 278/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.1900 - val_loss: 0.1950\n",
            "Epoch 279/300\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.1879 - val_loss: 0.1904\n",
            "Epoch 280/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.1783 - val_loss: 0.1791\n",
            "Epoch 281/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1548 - val_loss: 0.1591\n",
            "Epoch 282/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0991 - val_loss: 0.1590\n",
            "Epoch 283/300\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0824 - val_loss: 0.1677\n",
            "Epoch 284/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0967 - val_loss: 0.1639\n",
            "Epoch 285/300\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0978 - val_loss: 0.1736\n",
            "Epoch 286/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0812 - val_loss: 0.1604\n",
            "Epoch 287/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0694 - val_loss: 0.1571\n",
            "Epoch 288/300\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.0781 - val_loss: 0.1584\n",
            "Epoch 289/300\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.0786 - val_loss: 0.1670\n",
            "Epoch 290/300\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.0670 - val_loss: 0.1685\n",
            "Epoch 291/300\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.0669 - val_loss: 0.1658\n",
            "Epoch 292/300\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.0900 - val_loss: 0.1612\n",
            "Epoch 293/300\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.0917 - val_loss: 0.1692\n",
            "Epoch 294/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0455 - val_loss: 0.2050\n",
            "Epoch 295/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.0605 - val_loss: 0.1588\n",
            "Epoch 296/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.0496 - val_loss: 0.1685\n",
            "Epoch 297/300\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.1524 - val_loss: 0.1613\n",
            "Epoch 298/300\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.1222 - val_loss: 0.3297\n",
            "Epoch 299/300\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2261 - val_loss: 0.1887\n",
            "Epoch 300/300\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.1311 - val_loss: 0.1903\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efda09bf150>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#positive 측정 연습 - 무시\n",
        "score,acc=model.evaluate(test_data, test_label_data,\n",
        "                         batch_size=batch_size)\n",
        "print('Test performance: accuracy={0}, loss={1}'.format(acc, score))"
      ],
      "metadata": {
        "id": "YK0MPANuPiLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict([[0.681, 0.681, 0.754, 0.742, 0.712, 0.670, 0.742, 0.713]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiBYy3I6WBxM",
        "outputId": "4ce9ec93-760f-4ca3-d27d-ffa6c67af74a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00089567]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV5niVkKek75"
      },
      "source": [
        "#train\n",
        "jeju1_4_NA = [[0.781, 0.811, 0.853, 0.878, 0.781, 0.811, 0.853, 0.878],\n",
        "              [0.516, 0.530, 0.531, 0.555, 0.613, 0.531, 0.555, 0.613],\n",
        "              [0.636, 0.649, 0.717, 0.636, 0.649, 0.717, 0.750, 0.765],\n",
        "              [0.578, 0.578, 0.593, 0.656, 0.686, 0.695, 0.721, 0.721],\n",
        "              [0.552, 0.561, 0.603, 0.609, 0.630, 0.656, 0.665, 0.665]]\n",
        "\n",
        "#test\n",
        "jeju1_4_A = [[0.547, 0.549, 0.553, 0.547, 0.549, 0.553, 0.651, 0.651],\n",
        "             [0.520, 0.536, 0.520, 0.536, 0.554, 0.564, 0.590, 0.590],\n",
        "             [0.551, 0.571, 0.551, 0.571, 0.604, 0.649, 0.604, 0.649],\n",
        "             [0.550, 0.550, 0.553, 0.559, 0.579, 0.632, 0.634, 0.636],\n",
        "             [0.555, 0.562, 0.555, 0.562, 0.604, 0.625, 0.604, 0.625]]\n",
        "\n",
        "jeju2_4_NA = [[0.605, 0.758, 0.817, 0.830, 0.845, 0.715, 0.654, 0.690],\n",
        "              [0.685, 0.664, 0.606, 0.543, 0.572, 0.565, 0.524, 0.531],\n",
        "              [0.701, 0.613, 0.670, 0.633, 0.616, 0.683, 0.649, 0.653],\n",
        "              [0.673, 0.629, 0.704, 0.689, 0.617, 0.586, 0.644, 0.695],\n",
        "              [0.661, 0.630, 0.603, 0.561, 0.596, 0.620, 0.581, 0.552]]\n",
        "\n",
        "normal_4_NA = [[0.781, 0.781, 0.724, 0.726, 0.683, 0.811, 0.682, 0.773],\n",
        "               [0.593, 0.678, 0.679, 0.695, 0.718, 0.697, 0.726, 0.722],\n",
        "               [0.693, 0.696, 0.664, 0.682, 0.676, 0.677, 0.663, 0.744],\n",
        "               [0.681, 0.681, 0.754, 0.742, 0.712, 0.670, 0.742, 0.713],\n",
        "               [0.686, 0.656, 0.656, 0.667, 0.649, 0.661, 0.686, 0.741]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP-p63twbVFo"
      },
      "source": [
        "x_3_A = [[0.516, 0.556, 0.562, 0.573, 0.580, 0.602, 0.580, 0.602],\n",
        "         [0.534, 0.534, 0.543, 0.544, 0.547, 0.551, 0.560, 0.562],\n",
        "         [0.488, 0.557, 0.488, 0.557, 0.567, 0.571, 0.567, 0.571],\n",
        "         [0.566, 0.569, 0.569, 0.572, 0.579, 0.582, 0.623, 0.623],\n",
        "         [0.584, 0.584, 0.587, 0.592, 0.594, 0.595, 0.594, 0.595]]\n",
        "\n",
        "x_3_NA = [[0.691, 0.691, 0.721, 0.730, 0.738, 0.767, 0.768, 0.783],\n",
        "          [0.672, 0.751, 0.779, 0.792, 0.672, 0.751, 0.779, 0.792],\n",
        "          [0.625, 0.629, 0.634, 0.651, 0.660, 0.692, 0.660, 0.692],\n",
        "          [0.523, 0.523, 0.564, 0.576, 0.601, 0.633, 0.601, 0.633],\n",
        "          [0.577, 0.577, 0.583, 0.627, 0.716, 0.583, 0.627, 0.716]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcr_WVrgkThp"
      },
      "source": [
        "x_9_A = [[0.529, 0.530, 0.533, 0.533, 0.535, 0.546, 0.553, 0.569],\n",
        "         [0.496, 0.513, 0.519, 0.520, 0.529, 0.533, 0.535, 0.543],\n",
        "         [0.496, 0.496, 0.501, 0.506, 0.507, 0.523, 0.555, 0.555],\n",
        "         [0.543, 0.550, 0.555, 0.580, 0.597, 0.597, 0.597, 0.597],\n",
        "         [0.488, 0.490, 0.493, 0.508, 0.512, 0.517, 0.517, 0.517]]\n",
        "\n",
        "x_9_NA = [[0.782, 0.828, 0.782, 0.828, 0.829, 0.861, 0.861, 0.861],\n",
        "          [0.600, 0.600, 0.630, 0.631, 0.659, 0.665, 0.712, 0.716],\n",
        "          [0.658, 0.672, 0.693, 0.695, 0.699, 0.700, 0.751, 0.762],\n",
        "          [0.747, 0.747, 0.747, 0.747, 0.770, 0.785, 0.827, 0.827],\n",
        "          [0.532, 0.532, 0.555, 0.576, 0.590, 0.593, 0.648, 0.648]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCPW78YNloXF"
      },
      "source": [
        "x_10_A = [[0.004, 0.014,\t0.011,\t0.009,\t0.022],\n",
        "          [0.005,\t0.006,\t0.015,\t0.010, 0.022],\n",
        "          [0.005,\t0.012,\t0.016,\t0.010,\t0.016],\n",
        "          [0.006,\t0.013,\t0.020,\t0.012,\t0.022]]\n",
        "\n",
        "x_10_NA = [[0.506, 0.534, 0.543, 0.557, 0.566, 0.573, 0.585, 0.610],\n",
        "           [0.604, 0.604, 0.604, 0.641, 0.643, 0.670, 0.674, 0.674],\n",
        "           [0.609, 0.609, 0.609, 0.675, 0.698, 0.700, 0.742, 0.744],\n",
        "           [0.507, 0.507, 0.540, 0.566, 0.585, 0.624, 0.624, 0.624],\n",
        "           [0.558, 0.570, 0.665, 0.558, 0.570, 0.665, 0.672, 0.672]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFyKyEiPX5NV"
      },
      "source": [
        "x_18_A = [[0.502, 0.497, 0.504, 0.513, 0.522, 0.532, 0.532, 0.532],\n",
        "          [0.508, 0.509, 0.519, 0.539, 0.545, 0.565, 0.583, 0.625],\n",
        "          [0.504, 0.536, 0.538, 0.541, 0.542, 0.543, 0.556, 0.572],\n",
        "          [0.510, 0.520, 0.545, 0.559, 0.629, 0.636, 0.636, 0.636],\n",
        "          [0.482, 0.511, 0.526, 0.536, 0.539, 0.549, 0.549, 0.549]]\n",
        "\n",
        "x_18_NA = [[0.621, 0.621, 0.638, 0.652, 0.660, 0.678, 0.691, 0.711],\n",
        "           [0.587, 0.587, 0.620, 0.629, 0.688, 0.692, 0.725, 0.756],\n",
        "           [0.600, 0.607, 0.642, 0.668, 0.685, 0.735, 0.735, 0.735],\n",
        "           [0.738, 0.738, 0.748, 0.817, 0.837, 0.840, 0.925, 0.946],\n",
        "           [0.697, 0.697, 0.710, 0.725, 0.778, 0.785, 0.819, 0.819]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBtNKNG9msaJ"
      },
      "source": [
        "x_27_A = [[0.543, 0.555, 0.556, 0.565, 0.572, 0.618, 0.638, 0.638],\n",
        "          [0.586, 0.622, 0.655, 0.712, 0.730, 0.734, 0.742, 0.742],\n",
        "          [0.528, 0.528, 0.528, 0.531, 0.540, 0.625, 0.625, 0.625],\n",
        "          [0.533, 0.548, 0.549, 0.554, 0.555, 0.572, 0.606, 0.644],\n",
        "          [0.516, 0.522, 0.523, 0.531, 0.539, 0.539, 0.560, 0.657]]\n",
        "\n",
        "x_27_NA = [[0.587, 0.587, 0.599, 0.627, 0.652, 0.718, 0.652, 0.718],\n",
        "           [0.762, 0.780, 0.762, 0.780, 0.809, 0.848, 0.883, 0.936],\n",
        "           [0.491, 0.491, 0.505, 0.569, 0.575, 0.579, 0.631, 0.631],\n",
        "           [0.525, 0.525, 0.535, 0.588, 0.615, 0.535, 0.588, 0.615],\n",
        "           [0.608, 0.608, 0.667, 0.731, 0.749, 0.769, 0.785, 0.806]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_x_3_A = [[1.173, 1.122, 1.037, 1.060, 0.973, 0.774, 1.155, 0.952],\n",
        "           [1.186, 1.063, 1.027, 1.114, 1.013, 0.975, 1.015, 0.936],\n",
        "           [1.229, 1.229, 1.170, 1.170, 0.991, 1.126, 1.175, 0.909],\n",
        "           [1.236, 1.168, 1.068, 1.041, 1.094, 0.952, 1.178, 0.921],\n",
        "           [1.216, 1.085, 1.029, 1.142, 1.090, 0.861, 1.064, 0.920],\n",
        "           [1.176, 1.149, 1.210, 1.065, 0.903, 1.000, 0.985, 0.823],\n",
        "           [1.294, 1.213, 1.156, 1.171, 0.963, 0.595, 0.971, 0.946],\n",
        "           [1.121, 1.169, 1.311, 1.192, 1.017, 1.144, 1.023, 0.835]]\n",
        "\n",
        "n_x_3_NA = [[1.122, 1.179, 1.001, 1.183, 1.169, 1.227, 1.226, 1.245],\n",
        "            [1.105, 1.208, 1.045, 0.976, 1.181, 1.223, 1.285, 1.221],\n",
        "            [1.182, 1.219, 1.035, 1.185, 1.212, 1.142, 1.120, 1.220],\n",
        "            [1.108, 1.236, 1.067, 1.170, 1.267, 1.172, 1.288, 1.220],\n",
        "            [1.017, 1.214, 1.148, 1.103, 1.234, 1.217, 1.225, 1.226],\n",
        "            [0.082, 1.196, 1.150, 1.208, 1.215, 1.227, 1.222, 1.202],\n",
        "            [1.041, 1.267, 1.108, 1.263, 1.207, 1.194, 1.181, 1.148],\n",
        "            [1.121, 1.224, 1.129, 1.224, 1.083, 1.105, 1.273, 1.163]]"
      ],
      "metadata": {
        "id": "25XwANncSni8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_x_4_A = [[1.095, 0.965, 1.203, 1.037, 1.057, 1.052, 1.070, 0.814],\n",
        "           [1.125, 0.947, 1.215, 1.116, 0.968, 1.121, 0.994, 0.726],\n",
        "           [0.984, 1.179, 0.962, 1.053, 1.271, 1.147, 1.236, 1.216],\n",
        "           [0.917, 1.231, 1.098, 1.010, 1.253, 1.208, 1.231, 1.251],\n",
        "           [0.944, 1.330, 1.245, 1.007, 1.113, 1.278, 1.197, 1.177],\n",
        "           [1.002, 1.219, 1.217, 0.949, 1.172, 1.150, 1.103, 1.191],\n",
        "           [0.931, 1.176, 1.067, 0.991, 1.160, 1.174, 1.200, 1.240],\n",
        "           [0.997, 1.364, 1.259, 0.832, 1.185, 1.168, 1.167, 1.243]]\n",
        "\n",
        "n_x_4_NA = [[0.730, 1.302, 1.206, 1.210, 1.138, 1.352, 1.136, 1.289],\n",
        "            [1.151, 1.258, 0.992, 1.281, 1.377, 1.245, 1.244, 1.175],\n",
        "            [1.276, 1.500, 1.141, 1.350, 1.308, 1.402, 1.303, 1.267],\n",
        "            [1.044, 1.160, 1.156, 1.205, 1.227, 1.084, 1.111, 1.096],\n",
        "            [0.989, 1.130, 1.132, 1.159, 1.197, 1.161, 1.210, 1.203],\n",
        "            [1.155, 1.160, 1.106, 1.136, 1.127, 1.129, 1.105, 1.24],\n",
        "            [1.155, 1.135, 1.257, 1.237, 1.187, 1.117, 1.237, 1.189],\n",
        "            [1.143, 1.094, 1.094, 1.112, 1.082, 1.101, 1.144, 1.235]]"
      ],
      "metadata": {
        "id": "6QwBSIq-WKXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_x_9_A = [[1.073, 1.270, 0.841, 1.005, 1.116, 1.202, 1.258, 1.162],\n",
        "           [1.012, 1.246, 1.052, 1.005, 1.037, 1.199, 1.132, 1.173],\n",
        "           [1.053, 1.299, 1.335, 1.051, 1.141, 1.217, 1.270, 1.212],\n",
        "           [0.967, 1.352, 0.787, 1.160, 1.311, 1.204, 1.289, 1.192],\n",
        "           [1.024, 1.303, 1.214, 1.219, 1.258, 1.292, 1.327, 1.264],\n",
        "           [1.004, 1.148, 1.116, 1.044, 1.141, 1.116, 1.179, 1.110],\n",
        "           [0.997, 1.228, 0.843, 0.960, 1.112, 1.115, 1.240, 1.139],\n",
        "           [1.019, 1.212, 1.083, 0.892, 1.192, 1.188, 1.168, 1.179]]\n",
        "\n",
        "n_x_9_NA = [[1.011, 1.120, 0.986, 1.350, 1.201, 1.135, 1.157, 1.225],\n",
        "            [0.934, 1.268, 0.977, 1.229, 1.178, 1.224, 1.146, 1.108],\n",
        "            [0.975, 1.119, 1.204, 1.161, 1.052, 1.141, 1.122, 1.151],\n",
        "            [0.994, 1.155, 1.237, 1.235, 1.201, 1.241, 1.262, 1.152],\n",
        "            [0.980, 1.240, 1.230, 1.311, 1.265, 1.277, 1.270, 1.137],\n",
        "            [1.013, 1.370, 1.229, 1.279, 1.220, 1.347, 1.194, 1.328],\n",
        "            [0.950, 1.217, 1.184, 1.133, 1.133, 1.142, 1.172, 1.085],\n",
        "            [1.062, 1.172, 1.190, 1.209, 1.127, 1.155, 1.165, 1.126]]"
      ],
      "metadata": {
        "id": "xCu7fZ-lYRRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_x_10_A = [[1.046, 1.206, 1.186, 0.958, 1.203, 1.203, 1.240, 1.299],\n",
        "            [0.832, 1.207, 1.177, 0.957, 1.167, 1.095, 1.171, 1.099],\n",
        "            [0.764, 1.205, 1.154, 0.976, 1.177, 1.139, 1.253, 1.147],\n",
        "            [0.984, 1.130, 1.286, 0.929, 1.181, 1.145, 1.176, 0.993],\n",
        "            [0.916, 1.260, 0.641, 1.132, 1.085, 1.174, 1.214, 1.233],\n",
        "            [0.893, 1.201, 0.823, 1.182, 1.197, 1.237, 1.193, 1.110],\n",
        "            [0.916, 1.083, 0.973, 0.943, 1.128, 0.981, 1.074, 1.019],\n",
        "            [1.085, 1.233, 0.806, 1.030, 1.225, 1.175, 1.235, 1.209]]\n",
        "\n",
        "n_x_10_NA = [[0.918, 1.322, 1.262, 1.156, 1.289, 1.301, 1.117, 1.338],\n",
        "             [0.977, 1.277, 1.354, 1.185, 1.341, 1.216, 1.264, 1.255],\n",
        "             [0.955, 1.355, 1.252, 1.304, 1.207, 1.218, 1.124, 1.312],\n",
        "             [1.016, 1.254, 1.201, 1.167, 1.218, 1.255, 1.213, 1.193],\n",
        "             [1.039, 1.171, 1.271, 1.115, 1.009, 1.090, 1.160, 1.231],\n",
        "             [1.090, 1.216, 1.172, 1.095, 1.165, 1.108, 1.119, 1.159],\n",
        "             [1.032, 1.258, 1.238, 1.170, 1.175, 1.231, 1.204, 1.178],\n",
        "             [1.063, 1.133, 1.225, 1.235, 1.151, 1.277, 1.305, 1.290]]"
      ],
      "metadata": {
        "id": "LBS6C6QuYcSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_x_18_A = [[1.005, 1.312, 1.089, 1.053, 1.228, 1.220, 1.244, 1.103],\n",
        "            [0.951, 1.209, 0.764, 1.281, 1.111, 1.089, 1.197, 1.106],\n",
        "            [0.990, 1.217, 0.973, 1.160, 1.090, 1.196, 1.181, 1.147],\n",
        "            [0.994, 1.262, 0.975, 1.241, 1.300, 1.161, 1.218, 1.182],\n",
        "            [0.911, 1.123, 0.669, 1.244, 1.222, 1.166, 1.194, 1.101],\n",
        "            [1.127, 1.247, 0.748, 0.932, 1.236, 1.175, 1.153, 1.237],\n",
        "            [1.042, 1.150, 1.133, 0.992, 1.155, 1.273, 1.097, 1.018],\n",
        "            [1.099, 1.262, 0.709, 1.114, 1.152, 1.139, 1.207, 1.026]]\n",
        "\n",
        "n_x_18_NA = [[1.022, 1.164, 1.043, 1.167, 1.058, 0.963, 1.172, 1.041],\n",
        "             [1.153, 1.134, 1.071, 0.863, 1.136, 1.066, 1.165, 1.209],\n",
        "             [1.141, 1.315, 1.291, 1.179, 1.308, 1.104, 1.287, 0.815],\n",
        "             [1.035, 1.138, 0.886, 1.209, 1.245, 1.281, 1.275, 1.298],\n",
        "             [1.066, 1.207, 0.859, 1.188, 1.133, 1.088, 1.162, 1.251],\n",
        "             [1.050, 1.142, 1.076, 1.205, 1.170, 1.010, 1.139, 1.103],\n",
        "             [1.008, 1.206, 1.014, 1.253, 1.310, 1.321, 1.338, 1.105],\n",
        "             [1.150, 1.153, 1.137, 1.091, 1.289, 1.264, 1.200, 1.275]]"
      ],
      "metadata": {
        "id": "uTB_uFzdYd9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_x_27_A = [[0.958, 1.259, 0.771, 1.211, 1.217, 1.192, 1.252, 1.060],\n",
        "            [0.947, 1.360, 0.799, 1.255, 1.201, 1.238, 1.290, 1.058],\n",
        "            [0.892, 1.041, 1.016, 1.028, 0.987, 1.000, 0.991, 0.964],\n",
        "            [0.918, 1.185, 0.656, 1.106, 1.148, 1.119, 1.129, 1.160],\n",
        "            [1.079, 1.193, 0.871, 1.171, 1.090, 1.145, 1.194, 1.145],\n",
        "            [1.071, 1.242, 0.901, 1.181, 1.253, 1.101, 1.139, 1.126],\n",
        "            [0.998, 1.166, 1.025, 1.153, 1.145, 1.130, 1.200, 1.162],\n",
        "            [1.080, 1.234, 1.106, 1.234, 1.290, 1.213, 1.292, 1.146]]\n",
        "\n",
        "n_x_27_NA = [[1.036, 1.221, 1.046, 1.070, 1.144, 1.154, 1.192, 1.087],\n",
        "             [1.028, 1.187, 1.195, 1.170, 1.154, 1.206, 1.197, 1.215],\n",
        "             [1.057, 1.161, 1.148, 1.190, 1.257, 1.142, 1.293, 1.063],\n",
        "             [0.955, 1.166, 1.167, 1.224, 1.294, 1.282, 1.321, 1.061],\n",
        "             [1.095, 1.130, 1.059, 1.005, 1.079, 0.991, 0.974, 1.040],\n",
        "             [0.881, 1.176, 1.232, 1.164, 1.161, 1.162, 1.188, 1.309],\n",
        "             [1.158, 1.240, 1.266, 0.817, 1.180, 1.261, 1.187, 1.157],\n",
        "             [1.198, 1.269, 1.329, 0.939, 1.263, \t1.27, 1.231, 1.261]]"
      ],
      "metadata": {
        "id": "-V42CxZpYgS0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}